{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ostix360/ai-research/blob/main/encoder_to_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB1PgQKBPH_x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft torch\n",
        "!pip install -U git+https://github.com/huggingface/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from transformers import AutoModel, AutoModelForCausalLM\n",
        "from transformers.models.bert.modeling_bert import BertModel\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "class EncDec(nn.Module):\n",
        "    def __init__(self, enc_model: str, dec_model: str) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder: BertModel = AutoModel.from_pretrained(enc_model)\n",
        "        self.decoder: GPT2Model = AutoModelForCausalLM.from_pretrained(dec_model)\n",
        "        self.adapter = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "        self.decoder.wte = None # Remove token embeddings from decoder\n",
        "\n",
        "    def forward(self, input_ids, attention_mask,):\n",
        "        # Pass input through encoder\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Adapter brings the encoder outputs to the correct dimension for the decoder\n",
        "        encoder_hidden_states = self.adapter(encoder_outputs.last_hidden_state)\n",
        "        \n",
        "        # Pass adapter outputs and decoder_input_ids to the decoder\n",
        "        # In this case, \"encoder_hidden_states\" will be used as cross-attention \"encoder_attention_mask\"\n",
        "        # You have to manage them according to your use-case\n",
        "        decoder_outputs = self.decoder(inputs_embeds=encoder_hidden_states)\n",
        "        return decoder_outputs\n",
        "\n",
        "enc_model = \"bert-base-cased\"\n",
        "dec_model = \"gpt2\"\n",
        "model = EncDec(enc_model, dec_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "dataset = datasets.load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:10]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(enc_model)\n",
        "\n",
        "cutoff_len = 1024\n",
        "\n",
        "def tokenize_func(data):\n",
        "  return tokenizer(data[\"text\"], text_target=data[\"text\"], truncation=True, max_length=cutoff_len)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_func, batched=True, batch_size=1)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=2,\n",
        "    save_steps=2,\n",
        "    eval_steps=2,\n",
        "    warmup_steps=2,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "def freeze_params(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "freeze_params(model.encoder)\n",
        "freeze_params(model.decoder)\n",
        "\n",
        "nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "nb_trainable_params\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMKQDVd3RxFcr4EjLcZNToU",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
