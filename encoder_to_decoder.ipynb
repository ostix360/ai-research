{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ostix360/ai-research/blob/main/encoder_to_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB1PgQKBPH_x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets apache_beam peft torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 2.20MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 436M/436M [00:04<00:00, 92.7MB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 1.84MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 548M/548M [00:06<00:00, 86.2MB/s]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from transformers import AutoModel, AutoModelForCausalLM\n",
        "from transformers.models.bert.modeling_bert import BertModel\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "class EncDec(nn.Module):\n",
        "    def __init__(self, enc_model: str, dec_model: str) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder: BertModel = AutoModel.from_pretrained(enc_model)\n",
        "        self.decoder: GPT2Model = AutoModelForCausalLM.from_pretrained(dec_model)\n",
        "        self.adapter = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
        "        self.decoder.wpe = None # Remove position embeddings from decoder\n",
        "        self.decoder.wte = None # Remove token embeddings from decoder\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n",
        "        # Pass input through encoder\n",
        "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        # Adapter brings the encoder outputs to the correct dimension for the decoder\n",
        "        encoder_hidden_states = self.adapter(encoder_outputs.last_hidden_state)\n",
        "        \n",
        "        # Pass adapter outputs and decoder_input_ids to the decoder\n",
        "        # In this case, \"encoder_hidden_states\" will be used as cross-attention \"encoder_attention_mask\"\n",
        "        # You have to manage them according to your use-case\n",
        "        decoder_outputs = self.decoder(input_ids=decoder_input_ids,\n",
        "                                       attention_mask=decoder_attention_mask,\n",
        "                                       encoder_hidden_states=encoder_hidden_states)\n",
        "        return decoder_outputs\n",
        "\n",
        "enc_model = \"bert-base-cased\"\n",
        "dec_model = \"gpt2\"\n",
        "model = EncDec(enc_model, dec_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "dataset = datasets.load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:10]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(text, add_bos_token):\n",
        "    result = tokenizer.encode(text, truncation=True, max_length=cutoff_len)\n",
        "    # Check if the first two tokens are BOS\n",
        "    if len(result) >= 2 and result[:2] == [tokenizer.bos_token_id, tokenizer.bos_token_id]:\n",
        "        result = result[1:]\n",
        "\n",
        "    if not add_bos_token and result[0] == tokenizer.bos_token_id:\n",
        "        result = result[1:]\n",
        "    return result\n",
        "\n",
        "def tokenize(prompt, append_eos_token=False):\n",
        "    input_ids = encode(prompt, True)\n",
        "\n",
        "    if append_eos_token and input_ids[-1] != tokenizer.eos_token_id and len(input_ids) < cutoff_len:\n",
        "        input_ids.append(tokenizer.eos_token_id)\n",
        "\n",
        "    input_ids = [tokenizer.pad_token_id] * (cutoff_len - len(input_ids)) + input_ids\n",
        "    labels = [1] * len(input_ids)\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"labels\": labels,\n",
        "        \"attention_mask\": input_ids.ne(tokenizer.pad_token_id),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(enc_model)\n",
        "\n",
        "cutoff_len = 1024\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize, batched=True, batch_size=8)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=2,\n",
        "    save_steps=2,\n",
        "    eval_steps=2,\n",
        "    warmup_steps=2,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "def freeze_params(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "freeze_params(model.encoder)\n",
        "freeze_params(model.decoder)\n",
        "\n",
        "nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMKQDVd3RxFcr4EjLcZNToU",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
