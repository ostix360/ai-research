{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ostix360/ai-research/blob/main/encoder_to_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JB1PgQKBPH_x",
    "ExecuteTime": {
     "end_time": "2023-11-19T22:00:25.426701500Z",
     "start_time": "2023-11-19T21:59:55.988570200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (4.30.1)\n",
      "Requirement already satisfied: peft in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (0.4.0.dev0)\n",
      "Requirement already satisfied: torch in c:\\program files\\python311\\lib\\site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\program files\\python311\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\program files\\python311\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\program files\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in c:\\program files\\python311\\lib\\site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: accelerate in c:\\program files\\python311\\lib\\site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\program files\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\program files\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/datasets\n",
      "  Cloning https://github.com/huggingface/datasets to c:\\users\\grenouillon\\appdata\\local\\temp\\pip-req-build-xvi5_q3h\n",
      "  Resolved https://github.com/huggingface/datasets to commit c65315e4a8308f04fcb025039afe2a2e43b5684e\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (1.25.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (12.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from datasets==2.15.1.dev0) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from datasets==2.15.1.dev0) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from datasets==2.15.1.dev0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\program files\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.1.dev0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from datasets==2.15.1.dev0) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from datasets==2.15.1.dev0) (0.19.4)\n",
      "Requirement already satisfied: packaging in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python311\\lib\\site-packages (from datasets==2.15.1.dev0) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets==2.15.1.dev0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\program files\\python311\\lib\\site-packages (from aiohttp->datasets==2.15.1.dev0) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets==2.15.1.dev0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets==2.15.1.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets==2.15.1.dev0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets==2.15.1.dev0) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->datasets==2.15.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.15.1.dev0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python311\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.15.1.dev0) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets==2.15.1.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets==2.15.1.dev0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests>=2.19.0->datasets==2.15.1.dev0) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.15.1.dev0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python311\\lib\\site-packages (from pandas->datasets==2.15.1.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas->datasets==2.15.1.dev0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\program files\\python311\\lib\\site-packages (from pandas->datasets==2.15.1.dev0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\grenouillon\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.1.dev0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets 'C:\\Users\\grenouillon\\AppData\\Local\\Temp\\pip-req-build-xvi5_q3h'\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft torch\n",
    "!pip install -U git+https://github.com/huggingface/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:26:39.149993600Z",
     "start_time": "2023-11-20T22:26:14.057716Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.11.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.1.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.10.ln_cross_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.4.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.7.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight', 'h.2.ln_cross_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.9.ln_cross_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.6.ln_cross_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.3.ln_cross_attn.weight', 'h.0.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.1.crossattention.c_attn.bias', 'h.11.ln_cross_attn.bias', 'h.0.ln_cross_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.6.ln_cross_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.7.ln_cross_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.3.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModel, AutoModelForCausalLM, GPT2LMHeadModel\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "class EncDec(nn.Module):\n",
    "    def __init__(self, enc_model: str, dec_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder: BertModel = AutoModel.from_pretrained(enc_model)\n",
    "        self.decoder: GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(dec_model, add_cross_attention=True)\n",
    "        self.adapter = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Pass input through encoder\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Adapter brings the encoder outputs to the correct dimension for the decoder\n",
    "        encoder_hidden_states = self.adapter(encoder_outputs.last_hidden_state)\n",
    "        \n",
    "        # Pass adapter outputs and decoder_input_ids to the decoder\n",
    "        # In this case, \"encoder_hidden_states\" will be used as cross-attention \"encoder_attention_mask\"\n",
    "        # You have to manage them according to your use-case\n",
    "        decoder_outputs = self.decoder(input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, labels=labels)\n",
    "        return decoder_outputs\n",
    "        \n",
    "    \n",
    "    def _get_name(self):\n",
    "        return f\"{self.decoder._get_name()}\"\n",
    "\n",
    "enc_model = \"bert-base-cased\"\n",
    "dec_model = \"gpt2\"\n",
    "model = EncDec(enc_model, dec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T22:34:56.014564700Z",
     "start_time": "2023-11-20T22:34:51.187923800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 1364, 1110, 1103, 2223, 2370, 1104, 1103, 1214, 1107, 1103, 5916, 1105, 18123, 1811, 8729, 1116, 117, 1105, 2502, 1206, 1345, 1105, 1318, 119, 1135, 1110, 1141, 1104, 1300, 1808, 1106, 1138, 1476, 1552, 119, 1364, 1579, 3471, 1113, 1103, 1269, 1285, 1104, 1989, 1112, 1351, 117, 1105, 19148, 117, 1356, 1107, 13660, 1201, 119, 1364, 1579, 3769, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1382, 119, 1364, 112, 188, 4637, 1132, 1103, 7643, 153, 4490, 1105, 11291, 119, 2098, 20665, 4793, 1110, 1103, 9883, 119, 1109, 2764, 1104, 1103, 9883, 1110, 15025, 119, 1109, 17545, 1364, 2502, 1206, 1345, 1105, 1318, 117, 1543, 1122, 1103, 2223, 2370, 1104, 1103, 1214, 119, 1135, 1145, 2502, 1148, 1107, 1103, 1214, 1149, 1104, 1103, 1300, 1808, 1115, 1138, 1476, 1552, 117, 1112, 1340, 117, 1347, 1105, 1379, 1132, 1224, 1107, 1103, 1214, 119, 1364, 3471, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1351, 1451, 1214, 1105, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1356, 1107, 13660, 1201, 119, 1364, 3769, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1382, 1451, 1214, 117, 1112, 1296, 1168, 112, 188, 1314, 1552, 1132, 2839, 2588, 2277, 113, 23067, 1552, 114, 3966, 119, 1130, 1887, 1201, 117, 1364, 3816, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1357, 1104, 1103, 2166, 1214, 117, 1105, 1107, 13660, 1201, 117, 1318, 1104, 1103, 2166, 1214, 119, 1130, 1887, 1201, 117, 1364, 12531, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1351, 1104, 1103, 2166, 1214, 117, 1105, 1107, 13660, 1201, 117, 1428, 1105, 1357, 1104, 1103, 2166, 1214, 119, 1130, 1887, 1201, 2411, 1170, 1168, 1887, 1201, 117, 1364, 3816, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1356, 1104, 1103, 2166, 1214, 117, 1105, 1107, 13660, 1201, 1105, 1201, 2411, 1170, 1115, 117, 1364, 12531, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1356, 1104, 1103, 2166, 1214, 119, 1130, 1201, 2411, 1196, 1887, 1201, 117, 1364, 3816, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1347, 1105, 1382, 1104, 1103, 1378, 1214, 117, 1105, 1107, 1201, 2411, 1196, 13660, 1201, 117, 1340, 1104, 1103, 1378, 1214, 119, 1130, 1201, 2411, 1196, 1887, 1201, 117, 1364, 12531, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1347, 1104, 1103, 1378, 1214, 117, 1105, 1107, 1201, 2411, 1196, 13660, 1201, 117, 1345, 1105, 1340, 1104, 1103, 1378, 1214, 119, 1364, 1110, 170, 3450, 2370, 1107, 1103, 2579, 24371, 1105, 1126, 8929, 120, 2303, 2370, 1107, 1103, 2685, 24371, 119, 1130, 1296, 24114, 117, 1122, 1110, 1103, 13286, 4976, 1104, 1357, 1107, 1103, 1168, 119, 1135, 1110, 10527, 1112, 1106, 1187, 1364, 1400, 1157, 1271, 119, 138, 1887, 2749, 1110, 1115, 1122, 2502, 1121, 1103, 2911, 1937, 107, 170, 3365, 5817, 107, 117, 2764, 107, 1106, 1501, 107, 117, 7455, 1106, 4637, 2280, 1107, 3450, 119, 2543, 2749, 1110, 1115, 1103, 1271, 1180, 1435, 1121, 138, 7880, 13225, 3150, 117, 1103, 2414, 9659, 1104, 1567, 119, 1135, 1108, 2034, 1103, 1248, 2370, 1107, 1103, 1385, 2264, 26208, 117, 1196, 1103, 1838, 1104, 1103, 1207, 1214, 1108, 1508, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [101, 1364, 1110, 1103, 2223, 2370, 1104, 1103, 1214, 1107, 1103, 5916, 1105, 18123, 1811, 8729, 1116, 117, 1105, 2502, 1206, 1345, 1105, 1318, 119, 1135, 1110, 1141, 1104, 1300, 1808, 1106, 1138, 1476, 1552, 119, 1364, 1579, 3471, 1113, 1103, 1269, 1285, 1104, 1989, 1112, 1351, 117, 1105, 19148, 117, 1356, 1107, 13660, 1201, 119, 1364, 1579, 3769, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1382, 119, 1364, 112, 188, 4637, 1132, 1103, 7643, 153, 4490, 1105, 11291, 119, 2098, 20665, 4793, 1110, 1103, 9883, 119, 1109, 2764, 1104, 1103, 9883, 1110, 15025, 119, 1109, 17545, 1364, 2502, 1206, 1345, 1105, 1318, 117, 1543, 1122, 1103, 2223, 2370, 1104, 1103, 1214, 119, 1135, 1145, 2502, 1148, 1107, 1103, 1214, 1149, 1104, 1103, 1300, 1808, 1115, 1138, 1476, 1552, 117, 1112, 1340, 117, 1347, 1105, 1379, 1132, 1224, 1107, 1103, 1214, 119, 1364, 3471, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1351, 1451, 1214, 1105, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1356, 1107, 13660, 1201, 119, 1364, 3769, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1382, 1451, 1214, 117, 1112, 1296, 1168, 112, 188, 1314, 1552, 1132, 2839, 2588, 2277, 113, 23067, 1552, 114, 3966, 119, 1130, 1887, 1201, 117, 1364, 3816, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1357, 1104, 1103, 2166, 1214, 117, 1105, 1107, 13660, 1201, 117, 1318, 1104, 1103, 2166, 1214, 119, 1130, 1887, 1201, 117, 1364, 12531, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1351, 1104, 1103, 2166, 1214, 117, 1105, 1107, 13660, 1201, 117, 1428, 1105, 1357, 1104, 1103, 2166, 1214, 119, 1130, 1887, 1201, 2411, 1170, 1168, 1887, 1201, 117, 1364, 3816, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1356, 1104, 1103, 2166, 1214, 117, 1105, 1107, 13660, 1201, 1105, 1201, 2411, 1170, 1115, 117, 1364, 12531, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1356, 1104, 1103, 2166, 1214, 119, 1130, 1201, 2411, 1196, 1887, 1201, 117, 1364, 3816, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1347, 1105, 1382, 1104, 1103, 1378, 1214, 117, 1105, 1107, 1201, 2411, 1196, 13660, 1201, 117, 1340, 1104, 1103, 1378, 1214, 119, 1130, 1201, 2411, 1196, 1887, 1201, 117, 1364, 12531, 1113, 1103, 1269, 1285, 1104, 1103, 1989, 1112, 1347, 1104, 1103, 1378, 1214, 117, 1105, 1107, 1201, 2411, 1196, 13660, 1201, 117, 1345, 1105, 1340, 1104, 1103, 1378, 1214, 119, 1364, 1110, 170, 3450, 2370, 1107, 1103, 2579, 24371, 1105, 1126, 8929, 120, 2303, 2370, 1107, 1103, 2685, 24371, 119, 1130, 1296, 24114, 117, 1122, 1110, 1103, 13286, 4976, 1104, 1357, 1107, 1103, 1168, 119, 1135, 1110, 10527, 1112, 1106, 1187, 1364, 1400, 1157, 1271, 119, 138, 1887, 2749, 1110, 1115, 1122, 2502, 1121, 1103, 2911, 1937, 107, 170, 3365, 5817, 107, 117, 2764, 107, 1106, 1501, 107, 117, 7455, 1106, 4637, 2280, 1107, 3450, 119, 2543, 2749, 1110, 1115, 1103, 1271, 1180, 1435, 1121, 138, 7880, 13225, 3150, 117, 1103, 2414, 9659, 1104, 1567, 119, 1135, 1108, 2034, 1103, 1248, 2370, 1107, 1103, 1385, 2264, 26208, 117, 1196, 1103, 1838, 1104, 1103, 1207, 1214, 1108, 1508, 102]}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "t_dataset = datasets.load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[:1000]\")\n",
    "e_dataset = datasets.load_dataset(\"wikipedia\", \"20220301.simple\", split=\"train[-100:]\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(enc_model)\n",
    "\n",
    "cutoff_len = 512\n",
    "\n",
    "tokenizer(t_dataset[\"text\"][0], text_target=t_dataset[\"text\"][0], truncation=True, max_length=cutoff_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-20T22:34:56.018564600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column names are: ['input_ids', 'labels', 'attention_mask']\n",
      "Number of trainable parameters: 28939008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/500 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "from transformers import AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(enc_model)\n",
    "\n",
    "cutoff_len = 512\n",
    "\n",
    "def tokenize(prompt):\n",
    "\n",
    "    input_ids = tokenizer(prompt, truncation=True, max_length=cutoff_len)[\"input_ids\"]\n",
    "    input_ids = [tokenizer.pad_token_id] * (cutoff_len - len(input_ids)) + input_ids\n",
    "    labels = [1] * len(input_ids)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": input_ids.ne(tokenizer.pad_token_id),\n",
    "        }\n",
    "\n",
    "def tokenize_func(data):\n",
    "  return tokenize(data[\"text\"])\n",
    "\n",
    "tokenized_datasets = t_dataset.map(tokenize_func, remove_columns=[\"text\", \"title\", \"id\", \"url\"])\n",
    "e_tokenized_datasets = e_dataset.map(tokenize_func, remove_columns=[\"text\", \"title\", \"id\", \"url\"])\n",
    "\n",
    "\n",
    "print(f\"The column names are: {list(tokenized_datasets.features.keys())}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=5000,\n",
    "    eval_steps=2000,\n",
    "    warmup_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=e_tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "def freeze_params(model):\n",
    "    for param in model.parameters():\n",
    "        # train cross attention\n",
    "        # check params with name\n",
    "        param.requires_grad = False\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"crossattention\" in n:\n",
    "            p.requires_grad = True\n",
    "\n",
    "freeze_params(model.encoder)\n",
    "freeze_params(model.decoder)\n",
    "nb_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {nb_trainable_params}\")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKQDVd3RxFcr4EjLcZNToU",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
